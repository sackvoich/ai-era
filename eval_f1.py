import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
# >>> –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º confusion_matrix
from sklearn.metrics import f1_score, classification_report, accuracy_score, confusion_matrix

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ò (–ò–ó–ú–ï–ù–ò–¢–ï –≠–¢–ò –ó–ù–ê–ß–ï–ù–ò–Ø) ---

# –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –≤–∞—à–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
MODEL_PATH = "./back-front/models_4" # –ü—Ä–∏–º–µ—Ä: "./models/baseline_bert" –∏–ª–∏ –¥—Ä—É–≥–∞—è –ø–∞–ø–∫–∞

# –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º—É —Ç–µ—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É
TEST_CSV_PATH = "profanity_dataset_500.csv" # –§–∞–π–ª –Ω–∞ 500 —Å—Ç—Ä–æ–∫

# –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –≤ –≤–∞—à–µ–º CSV
TEXT_COLUMN = "text"
LABEL_COLUMN = "label"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
MAX_LENGTH = 256
BATCH_SIZE = 16

# --- –ö–û–ù–ï–¶ –ù–ê–°–¢–†–û–ï–ö ---


def evaluate_model(model_path, csv_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ CSV —Ñ–∞–π–ª–µ
    –∏ –≤—ã–≤–æ–¥–∏—Ç –º–µ—Ç—Ä–∏–∫–∏, –≤–∫–ª—é—á–∞—è –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫.
    """
    print("="*50)
    print(f"–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏: {model_path}")
    print(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏: {csv_path}")
    print("="*50)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    try:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model.to(device)
    except OSError:
        print(f"!!! –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å –ø–æ –ø—É—Ç–∏: {model_path}")
        return

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏...")
    try:
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        print(f"!!! –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏: {csv_path}")
        return
        
    df.dropna(subset=[TEXT_COLUMN, LABEL_COLUMN], inplace=True)
    df[TEXT_COLUMN] = df[TEXT_COLUMN].astype(str)
    
    dataset = Dataset.from_pandas(df)

    def tokenize_function(examples):
        return tokenizer(
            examples[TEXT_COLUMN], padding="max_length", truncation=True, max_length=MAX_LENGTH
        )

    print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    tokenized_dataset = dataset.map(tokenize_function, batched=True)

    print("–ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
    training_args = TrainingArguments(
        output_dir="./temp_eval_results", per_device_eval_batch_size=BATCH_SIZE, fp16=torch.cuda.is_available()
    )
    trainer = Trainer(model=model, args=training_args)
    
    raw_predictions = trainer.predict(tokenized_dataset)
    logits = raw_predictions.predictions
    predicted_labels = np.argmax(logits, axis=1)
    true_labels = df[LABEL_COLUMN].values

    # --- 6. –†–∞—Å—á–µ—Ç –∏ –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ ---
    print("\n--- –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò ---")

    # >>> –ù–û–í–û–ï: –í—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ (Confusion Matrix) <<<
    print("\n--- –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (TP/FP/TN/FN) ---")
    cm = confusion_matrix(true_labels, predicted_labels)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è TN, FP, FN, TP
    # –ú–∞—Ç—Ä–∏—Ü–∞ –∏–º–µ–µ—Ç –≤–∏–¥: [[TN, FP], [FN, TP]]
    tn, fp, fn, tp = cm.ravel()

    # –†–∏—Å—É–µ–º –∫—Ä–∞—Å–∏–≤—É—é —Ç–∞–±–ª–∏—Ü—É
    print(f"               | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: –ù–ï–¢ | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: –ï–°–¢–¨")
    print(f"-----------------------------------------------------")
    print(f" –†–µ–∞–ª—å–Ω–æ: –ù–ï–¢    | {tn:^16} | {fp:^16}")
    print(f" –†–µ–∞–ª—å–Ω–æ: –ï–°–¢–¨   | {fn:^16} | {tp:^16}")
    print(f"-----------------------------------------------------")

    print("\n–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞:")
    print(f"  - True Negative  (TN): {tn:<5} | –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞, —á—Ç–æ –º–∞—Ç–∞ –ù–ï–¢.")
    print(f"  - False Positive (FP): {fp:<5} | –û—à–∏–±–∫–∞! –ú–æ–¥–µ–ª—å –Ω–∞—à–ª–∞ –º–∞—Ç —Ç–∞–º, –≥–¥–µ –µ–≥–æ –ù–ï–¢ (–õ–æ–∂–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞).")
    print(f"  - False Negative (FN): {fn:<5} | –û—à–∏–±–∫–∞! –ú–æ–¥–µ–ª—å –ù–ï –Ω–∞—à–ª–∞ –º–∞—Ç —Ç–∞–º, –≥–¥–µ –æ–Ω –ï–°–¢–¨ (–ü—Ä–æ–ø—É—Å–∫).")
    print(f"  - True Positive  (TP): {tp:<5} | –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞, —á—Ç–æ –º–∞—Ç –ï–°–¢–¨.")
    
    # –ì–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è
    f1_binary = f1_score(true_labels, predicted_labels, average='binary')
    print(f"\nüéØ –ì–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ (F1-score –¥–ª—è –∫–ª–∞—Å—Å–∞ 1): {f1_binary:.4f}")
    
    accuracy = accuracy_score(true_labels, predicted_labels)
    print(f"   –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy): {accuracy:.4f}\n")
    
    # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º
    report = classification_report(
        true_labels,
        predicted_labels,
        target_names=['–ö–ª–∞—Å—Å 0 (–Ω–µ—Ç –º–∞—Ç–∞)', '–ö–ª–∞—Å—Å 1 (–µ—Å—Ç—å –º–∞—Ç)']
    )
    print("–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç:")
    print(report)
    print("="*50 + "\n")


if __name__ == '__main__':
    evaluate_model(MODEL_PATH, TEST_CSV_PATH)